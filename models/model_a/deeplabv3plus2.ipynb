{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+ Versão Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/topics/deeplabv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tensorflow/models.git\n",
    "%cd models/research/deeplab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install tf_slim\n",
    "!pip install tensorflow==2.4.1\n",
    "!pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc --python_out=. ./deeplab/protos/*.proto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/models')\n",
    "sys.path.append('/content/models/research')\n",
    "sys.path.append('/content/models/research/slim')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tratar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "  --logtostderr \\\n",
    "  --training_number_of_steps=50000 \\\n",
    "  --train_split=\"train\" \\\n",
    "  --model_variant=\"xception_65\" \\\n",
    "  --atrous_rates=6 \\\n",
    "  --atrous_rates=12 \\\n",
    "  --atrous_rates=18 \\\n",
    "  --output_stride=16 \\\n",
    "  --decoder_output_stride=4 \\\n",
    "  --train_crop_size=513 \\\n",
    "  --train_crop_size=513 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --dataset=\"dolphin\" \\\n",
    "  --tf_initial_checkpoint=\"path_to_initial_trained_model/deeplabv3_pascal_train_aug/model.ckpt\" \\\n",
    "  --train_logdir=\"path/to/training_log\" \\\n",
    "  --dataset_dir=\"path/to/training_dataset\" \\\n",
    "  --fine_tune_batch_norm=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Definir o modelo DeepLabV3+ com ResNet34 como backbone\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name='resnet34',\n",
    "    encoder_depth=5,\n",
    "    encoder_weights='imagenet',\n",
    "    encoder_output_stride=16,\n",
    "    decoder_channels=256,\n",
    "    decoder_atrous_rates=(12, 24, 36),\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    "    upsampling=4,\n",
    "    aux_params=None\n",
    ")\n",
    "\n",
    "# Configurar hiperparâmetros de treinamento\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 50\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Definir a função de perda\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Configurar o DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "# Definir transformações\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Carregar o dataset\n",
    "train_dataset = VOCSegmentation(root='./data', year='2012', image_set='train', download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "# Função de treino\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Treinar o modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terceira alternativa\n",
    "https://github.com/VainF/DeepLabV3Plus-Pytorch\n",
    "\n",
    "Construção do modelo usando o backbone pré treinado e construindo o deeplabv3+ do zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
